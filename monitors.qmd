---
title: "Air Monitor Data Wrangling and Visualizations"
format: html
---

## Libraries

- tidyverse handles all the data wrangling
- readxl is for reading in the manually-edited master list of air monitors
- tidygeocoder adds coordinates based on the addresses of monitors

```{r}
#| label: Libraries

library(tidyverse)
library(readxl)
library(tidygeocoder)
library(writexl)
```

## Monitor Master List

- Adds a full address column (minus zip code) which the geocoder needs to work
- Does the geocoding to add the coordinates (it kind of just neatly adds a lat and long column which is cool)
- Manually fills in missing coordinates
- Makes a new, simplified dataset that can be used to join with the real datasets to add coordinates
- Problem: it doesn't recognize all of the addresses
  - osm method (default) can't find La Canada, Teakwood, W Orchard, Cricket
  - census method can't find Cinnamon
  - right now it uses census method and adds cinnamon manually
    - 14995 Cinnamon Dr, Fontana, CA (34.03781, -117.4836)
    - 18449 La Canada Ct, Bloomington, CA (34.06047, -117.4018)
    - 1063 S Teakwood Ave, Bloomington, CA (34.08215, -117.3895)
    - 1477 W Orchard Street, Bloomington, CA (34.09003, -117.4004)
    - 19338 Cricket Ct, Bloomington, CA (34.03924, -117.3826)
  - the problem is when new monitors get added they go to the top so Cinnamon Dr isn't always going to be the 33rd row
  - I would like for it to figure out which row is Cinnamon and add it there
  - Better yet using the geocoder with another method to add it to the missing rows

```{r}
#| label: Monitor Master List

monitors <- read_excel("data/airmonitors_addresses.xlsx", 1)

monitors <- monitors |>
  mutate(full_address = str_c(address, 
                              city, 
                              "CA", 
                              sep = ", "))

# Searches for coordinates for each address
monitors <- geocode(monitors, 
                    address = full_address, 
                    method = 'census') 

#monitors_osm <- geocode(monitors, address = full_address) |>
  #select(monitor, serial, address, city, location, full_address, lat, long)

# One of the addresses doesn't work with the census method so this fills it in manually
monitors$lat[33] <- 34.03781
monitors$long[33] <- -117.4836

# The main data spreadsheets have the serial numbers in all lowercase so this matches it to that
coords <- monitors |> 
  mutate(Source = str_to_lower(serial)) |>
  select(lat, 
         long, 
         location,
         Source)
```

## Full Air Quality Dataset

- Needs input of the dataset parameters (dates and frequency), then it should be able to handle the creation of the main dataset on its own
- Recognizes the folder of all the monitor sheets in the data folder
- Figures out how many files are in that folder (for some reason length() returns it with an L at the end so it extracts just the number)
- Adds each small spreadsheet to one big spreadsheet
- Imports data into RStudio
- Adds coordinate columns
- Should use lubridate to add a month column/whatever else if need be

```{r}
#| label: Full Air Quality Dataset
#| message: false
#| warning: false

# Unzip the IQAir data export into the RRC_IQAir/data/ folder.

# Fill in the bounds of the dataset and the rate of data collection. 
# end_date should be the date after (if the data ends on August 31 it should be 01Sep25)
start_date <- '29Jan25'
end_date <-  '29Sep25'
frequency <- 'monthly'
data_info <- str_c(start_date, 
                   '-',
                   end_date, 
                   '_', 
                   frequency)

dataset_folder <- str_c('data/IQAir_Export_raw_devices_', 
                        data_info)

# Gets the number of files in the dataset folder (so it knows how many times to run the import)
monitor_count <- as.numeric(str_extract(length(list.files(dataset_folder)), 
                                        '\\d+'))
#monitor_count <- as.numeric(str_extract(length(list.files('/data')), '\\d+'))

# Runs 58 times. Goes through each serial number in the master list, finds the corresponding csv file, adds it to one big table one after the other.
import_data <- function(dataset_folder) {
  for (i in 1:monitor_count) {
    current_directory <- str_c(dataset_folder, 
                               '/IQAir_raw_', 
                               coords$Source[i], 
                               '_', 
                               data_info, 
                               '.csv')
    if (i == 1) {
      big_table <- read_csv(current_directory)
    } else {
      current_table <- read_csv(current_directory)
      big_table <- bind_rows(big_table, 
                             current_table)
    }
  }
  return(big_table)
}

data <- import_data(dataset_folder)

data_prepared <- data |>
  left_join(coords, by = 'Source') |>
  mutate(datetime = mdy_hm(`Datetime_start(UTC)`)) |>
  mutate(month = month(datetime)) |>
  mutate(color = cut(`AQI US`,
                      labels = c('green', 'yellow', 'orange', 'red', 'purple', 'maroon'),
                      breaks <- c(0, 50, 100, 150, 200, 300, Inf),
                      right = T))

monitors_activity <- data_prepared |>
  group_by(Source) |>
  summarize(months = n()) |>
  mutate(Source = str_to_upper(Source))


monitors_activity_full <- monitors |>
  left_join(monitors_activity, join_by(serial == Source)) 

monitors_activity_full |>
  write_xlsx("data/monitors_activity_full.xlsx")

```

## Plots

- Very basic timeseries scatterplots
- TBD

```{r}
#| label: Plots

data_prepared |>
  ggplot(aes(x = month, 
             y = `AQI US`)) +
  geom_point() +
  geom_smooth(se = F)

data_prepared |>
  ggplot(aes(x = month, 
             y = `AQI US`)) +
  geom_point() +
  geom_line(aes(group = Source)) +
  geom_smooth(se = F)

average <- data_prepared |>
  filter(location == 'outdoor') |>
  summarize(x = "Inland Empire 2025",
            average_pm25 = mean(`PM2.5 (ug/m3)`),
            average_pm10 = mean(`PM10 (ug/m3)`),
            average_aqi = mean(`AQI US`)) |>
  bind_rows(data.frame(x = c('WHO Guideline', 'US 2024 Average', 'EPA Standard'),
                       average_pm25 = c(5, 7.08, 9.0),
                       average_pm10 = c(15, NA, NA),
                       average_aqi = c(NA, 39, NA))) |>
  mutate(average_pm25 = round(average_pm25, 2)) |>
  mutate(x = fct_reorder(x, average_pm25))
  
  
# WHO PM2.5 guideline: 5.00 micrograms/cu m
# US Average: 7.08 
# IE Average: 15.02
  
average |>
  ggplot() +
  geom_col(aes(x = x,
               y = average_pm25,
               fill = x)) +
  geom_text(aes(x = x,
                y = average_pm25-1,
                label = average_pm25), 
            size = 5) +
  labs(y = bquote('Annual PM2.5 Average' ~(mu*g/m^3)),
       title = 'Particulates in the Inland Empire are 3 Times the Healthy Amount',
       subtitle = 'IQAir Monitor Data, Public Health & Environmental Standards') +
  scale_fill_manual(values = c('#55cc22', '#eedd44', '#e9bb44', '#dd9944')) + 
  theme_bw() +
  theme(axis.title.x = element_blank(),
        legend.position = "none")

c('#eeaa33','#eeee33','#33aa33')
```

## Maps

### To Do
- Label each marker with air quality
- Create a shiny app
- Add a time slider to be able to see the data change over time
- Use the API to have it update in real time
- Host a shiny app online

```{r}
library(leaflet)
library(leaflet.extras)
library(sf)

colors <- c('green', 'yellow', 'orange', 'red', 'purple', 'maroon')
#bins <- c(0, 50, 100, 150, 200, 300)


WH.url <- 'https://raw.githubusercontent.com/RadicalResearchLLC/WarehouseMap/main/WarehouseCITY/geoJSON/comboFinal.geojson'

warehouses <- st_read(WH.url) |>  
  st_transform(crs = 4326)

aqi_pal = colorFactor(colors, domain = data_prepared$color)

leaflet(data_prepared) |>
  addTiles() |>
  addLabelOnlyMarkers(
    lat = ~lat, lng = ~long,
    label = ~as.character(`AQI US`),
    labelOptions = labelOptions(
      noHide = TRUE,
      direction = "center",
      textOnly = FALSE,
      style = list(
        "color" = "white",
        "background" = '#dd9944',  
        "border-radius" = "50%",
        "text-align" = "center"
      )))

'#dd9944'

data_prepared |>
  filter(location == 'outdoor') |>
  leaflet() |>
  addTiles() |>
  addCircleMarkers(
    lat = ~lat, lng = ~long,
    label = ~as.character(`AQI US`),
    color = ~aqi_pal(color)) |>
  addLabelOnlyMarkers(
    lat = ~lat, lng = ~long,
    label = ~as.character(`AQI US`),
    labelOptions = labelOptions(
      noHide = TRUE,
      direction = "center",
      textOnly = T,
      style = list(
        "color" = ~color,
        #"background" = '#dd9944',  
        "border-radius" = "50%",
        "text-align" = "center"
      ))) |>
  addPolygons(data = warehouses,
              color = 'black',
              weight = 1)


```

```{r}
## Summary stats and some plots
library(psych)

#table(data_prepared$month) #July through September are most common months 
#summary(filter(data_prepared, month >= 7 & month <=9))

describeBy(data_prepared[5], group = data_prepared$month, mat = T)

## Mean AQI US and PM2.5 per month per location (indoor or outdoor)
stats <- describeBy(data_prepared[c(5,7)], 
                    list(data_prepared$month, 
                         data_prepared$location), 
                    mat = T)

stats_clean <- stats |>
  select(month = group1,
         location = group2,
         variable = vars,
         mean) |>
  mutate(variable = recode(
    variable, 
    `1` = "AQI US mean",
    `2` = "PM2.5 mean")) 
    
stats_wide <- stats_clean |>
  pivot_wider(
    names_from = variable,
    values_from = mean, 
  )


stats_wide |> 
  ggplot(aes(x = month,
             y = `AQI US mean`,
             fill = location)) + 
  geom_col(position = "dodge") 

stats_wide |> 
  ggplot(aes(x = month,
             y = `PM2.5 mean`,
             fill = location)) + 
  geom_col(position = "dodge") 
           
```

